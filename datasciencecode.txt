import pandas as pd
 test=pd.read_csv("C:\\Users\\MANISH\\Desktop\\gradebook.csv")
print(test)
print("Highest score in java course:",test["Java"].max())
print("Highest score in python course:",test["python"].max())
print("Highest score in total course:",test["total"].max())
print("Highest score in java course:",test["Java"].min())
print("minimum score in java course:",test["python"].min())
print("minimum total :",test["total"].min())


import pandas as pd
>>> names = {'python': ['89','23','34','43','66','45','67','98']}
>>> df = pd.DataFrame(names,columns=['python'])
>>> df['pythonresult'] = df['python'].apply(lambda x: 'Fail' if x == '23' else 'Pass')
cross:tab
import pandas as pd
test=pd.read_csv("C:\\Users\\MANISH\\Desktop\\gradebook.csv")
test
pd.crosstab(test.result,test.slot)

 import pandas as pd
import scipy.stats as stats
import researchpy as rp
mport researchpy as rp
test=pd.read_csv("C:\\Users\\MANISH\\Desktop\\studentdetails.csv")
rp.summary_cat(df[["result", "slot"]])
crosstab = pd.crosstab(test["result"], test["slot"])
 crosstab
stats.chi2_contingency(crosstab)
crosstab, test_results, expected = rp.crosstab(test
					["result"], test["slot"],
                                               test= "chi-square",
                                               expected_freqs= True,

test_results
expected


import pandas as pd
>>> test=pd.read_csv("C:\\Users\\MANISH\\Desktop\\gradebook.csv")
>>> test=pd.read_csv("C:\\Users\\MANISH\\Desktop\\gradebook.csv")
>>> print(test)
   pg.corr(x=test['Java'],y=test['python'])

total_data = pd.concat([data0], ignore_index=True)
type(total_data)
total_data['slot'].dtype
#tuple 
X = ('1','2','3','4','5','6','7','8','9') 
type(X)
#list 
X1 = ['1','2','3','4','5','6','7','8','9'] 
type(X1)


import os 
os.getcwd() 
os.chdir(r'C:\Users\varsh\Documents') 
os.getcwd() 
os.listdir() 
os.path.exists("gradebook.csv") 
import pandas as pd 
import matplotlib.pyplot as plt 
df= pd.read_csv('gradebook.csv') 
df.boxplot(by ='TOTAL', column =['NAME'], grid = False) 

import os 
os.getcwd() 
os.chdir(r'C:\Users\varsh\Documents') 
os.getcwd() 
os.listdir() 
os.path.exists("STUDENTS DATA.csv") 
import pandas as pd 
import matplotlib.pyplot as plt 
df= pd.read_csv('STUDENTS DATA.csv') 
x = df['TOTAL'] 
plt.hist(x, bins=[0,25,50,100,150,200,250,300])

import os 
os.getcwd() 
os.chdir(r'C:\Users\varsh\Documents') 
os.getcwd() 
os.listdir() 
os.path.exists("STUDENTS DATA.csv") 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns 
data=pd.read_csv('STUDENTS DATA.csv') 
dataplot=sns.heatmap(data.corr())  
plt.show() 

import os 
os.getcwd() 
os.chdir(r'C:\Users\varsh\Documents') 
os.getcwd() 
os.listdir() 
os.path.exists("STUDENTS DATA.csv") 
import pandas as pd 
import matplotlib.pyplot as plt
df = pd.read_csv('STUDENTS DATA.csv') 
x = df['CUT-OFF'] 
y = df['TOTAL'] 
plt.xlabel('CUT-OFF', fontsize=18) 
plt.ylabel('TOTAL', fontsize=16) 
plt.scatter(x, y) 
plt.plot(x, y) 

import os 
os.getcwd() 
os.chdir(r'C:\Users\varsh\Documents') 
os.getcwd() 
os.listdir() 
os.path.exists("STUDENTS DATA.csv")
import pandas as pd 
import matplotlib.pyplot as plt 
df = pd.read_csv('STUDENTS DATA.csv') 
x = df['CUT-OFF'] 
y = df['TOTAL'] 
plt.xlabel('CUT-OFF', fontsize=10) 
plt.ylabel('TOTAL', fontsize=10) 
plt.plot(x, y) 


import warnings
warnings.filterwarnings('ignore')
df = pd.read_csv('E:\Vaishnavi\gradebook.csv')
df.shape
total_studentname = df.shape[0]
print("Total Number of students  :", total_studentname)
df['cgpa'].value_counts()
cgpa = (df['cgpa'] >= 80).sum()
print("No of students with above 80 cgpa:",cgpa)
probability = (cgpa/total_studentname)*100
print('Probability of picking a student with above 80 cgpa: {0:.2f}'.format(probability )+'%')

cond_prob_cgpa= (cgpa/total_studentname) * ((cgpa - 1)/(total_studentname - 1)) 
print("The Probability of Picking a student with above 80cgpa and again picking a student with above 80cgpa is {0:.3f}".
      format(cond_prob_cgpa*100))

from functools import partial
# number of samples to average over.
n=np.array([1, 5, 10,200])
# number of times samples of size n are taken. 
N = 1000
# number of bin boundaries on plots
nobb=101
dist=[partial(np.random.random),]
# lets define the title names.
title_names=["Flat"]
# ranges of the three distributions
drange=np.array([[0,1]]) 
# means of the three distributions
means=np.array([0.5])
# variances of the three distributions
var=np.array([1/12]) 
# generates random samples in the specified ranges for the respective distributions.
binrange=np.array([np.linspace(p,q,nobb) for p,q in drange]) 
ln,ld=len(n),len(dist)
plt.figure(figsize=((ld*4)+1,(ln*2)+1))
 # loop over number of n samples to average over
for i in range(ln):
     # loop over the different distributions
    for j in range(ld):
        plt.subplot(ln,ld,i*ld+1+j)
        plt.hist(np.mean(dist[j]((N,n[i])),1),binrange[j])
        plt.xlim(drange[j])
        if j==0:
            plt.ylabel('n=%i' % n[i],fontsize=15)        
        if i==0:
            plt.title(title_names[j], fontsize=15)
        else:
            clt=(1/(np.sqrt(2*np.pi*var[j]/n[i])))*np.exp(-(((binrange[j]-means[j])**2)*n[i]/(2*var[j])))
            plt.plot(binrange[j],clt,'y',linewidth=2)     
plt.show()

import scipy.stats as stats
import math
# lets seed the random values
np.random.seed(10)
# lets take a sample size
sample_size = 1000
sample = np.random.choice(a= df['cgpa'],
                          size = sample_size)
sample_mean = sample.mean()
# Get the z-critical value*
z_critical = stats.norm.ppf(q = 0.95)  
 # Check the z-critical value  
print("z-critical value: ",z_critical)                                
# Get the population standard deviation
pop_stdev = df['cgpa'].std()  
# checking the margin of error
margin_of_error = z_critical * (pop_stdev/math.sqrt(sample_size)) 

# defining our confidence interval
confidence_interval = (sample_mean - margin_of_error,
                       sample_mean + margin_of_error)  

# lets print the results
print("Confidence interval:",end="")
print(confidence_interval)
print("True mean: {}".format(df['cgpa'].mean()))
np.random.seed(12)
sample_size = 500
intervals = []
sample_means = []
for sample in range(25):
    sample = np.random.choice(a= df['cgpa'], size = sample_size)
    sample_mean = sample.mean()
    sample_means.append(sample_mean)
     # Get the z-critical value* 
    z_critical = stats.norm.ppf(q = 0.97)         
    # Get the population standard deviation
    pop_stdev = df['cgpa'].std()  
    stats.norm.ppf(q = 0.025)
    margin_of_error = z_critical * (pop_stdev/math.sqrt(sample_size))
    confidence_interval = (sample_mean - margin_of_error,
                           sample_mean + margin_of_error)  
    intervals.append(confidence_interval)
plt.figure(figsize=(13, 9))

plt.errorbar(x=np.arange(0.1, 25, 1), 
             y=sample_means, 
             yerr=[(top-bot)/2 for top,bot in intervals],
             fmt='o')

plt.hlines(xmin=0, xmax=25,
           y=df['cgpa'].mean(), 
           linewidth=2.0,
           color="red")
plt.title('Confidence Intervals for 25 Trials', fontsize = 20)
plt.show()

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.cluster import AgglomerativeClustering
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
from sklearn.pipeline import make_pipeline 
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
#for avoiding warnings
import warnings
warnings.filterwarnings('ignore')
df = pd.read_csv('E:\Vaishnavi\gradebook.csv')
df=df.drop(' rollno',axis=1)
train_X, test_X = train_test_split(df, test_size=0.2, random_state=42)
print(len(train_X), "train +", len(test_X), "test")
df2 = train_X.copy()
le = LabelEncoder()
le.fit(df2.result)

le.classes_ 
df2.loc[:,'result'] = le.transform(df2.result)
df2.head(6)
scaler = StandardScaler()
scaler.fit(df2)
data_scaled = scaler.transform(df2)
data_scaled[0:3]
pca = PCA()
# fit PCA
pca.fit(data_scaled)
features = range(pca.n_components_) 
features 
data_pca = pca.transform(data_scaled) 
data_pca.shape 
pca.explained_variance_ratio_ 
plt.bar(features, pca.explained_variance_ratio_) 
plt.xticks(features) 
plt.ylabel('variance') 
plt.xlabel('PCA feature') 
plt.show() 
pca2 = PCA(n_components=2, svd_solver='full') 
# fit PCA 
pca2.fit(data_scaled) 
# PCA transformed data 
data_pca2 = pca2.transform(data_scaled) 
data_pca2.shape

xs = data_pca2[:,0] 
ys = data_pca2[:,1] 
#zs = train_X.iloc[:,2] 
plt.scatter(ys, xs) 
#plt.scatter(ys, zs, c=labels) 
plt.grid(False) 
plt.title('Scatter Plot of student data') 
plt.xlabel('PCA-01') 
plt.ylabel('PCA-02') 
plt.show()

k=4 
kmeans = KMeans(n_clusters=k, init = 'k-means++',random_state = 42) 
pipeline = make_pipeline(scaler, pca2, kmeans) 
#pipeline = make_pipeline(kmeans) 
# fit the model to the scaled dataset 
model_fit = pipeline.fit(df2) 
model_fit
labels = model_fit.predict(df2) //Assigning labels 
labels
# lets add the clusters to the dataset 
train_X['Clusters'] = labels 
# Number of data points for each feature in each cluster 
train_X.groupby('Clusters').count()

xs = data_pca2[:,0] 
ys = data_pca2[:,1] 
#zs = train_X.iloc[:,2] 
plt.scatter(ys, xs,c=labels) 
#plt.scatter(ys, zs, c=labels) 
plt.grid(False) 
plt.title('Scatter Plot of student data') 
plt.xlabel('PCA-01') 
plt.ylabel('PCA-02') 
plt.show()
centroids = model_fit[2].cluster_centers_ 
Centroids

model_fit[2].inertia_ 
df_new = test_X.copy() 
# predict the labels 
le.fit(df_new.result) 
#update df2 with transformed values of gender 
df_new.loc[:,'result'] = le.transform(df_new.result) 
labels_test = model_fit.predict(df_new) 
labels_testplt.show() 
test_X['Clusters'] = labels_test 
# Number of data points for each feature in each cluster 
test_X.groupby('Clusters').count() 
query = (test_X['Clusters']==1) 
test_X[query]
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.cluster import AgglomerativeClustering
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
from sklearn.pipeline import make_pipeline 
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
#for avoiding warnings
import warnings
warnings.filterwarnings('ignore')
data = pd.read_csv('E:\Vaishnavi\gradebook.csv')
data.rename(columns={'total':'total','result':'result'},inplace=True)
for i,col in enumerate(data.columns):
 print((i+1),'. columns is :',col)
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.cluster import AgglomerativeClustering
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
from sklearn.pipeline import make_pipeline 
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
#for avoiding warnings
import warnings
warnings.filterwarnings('ignore')
df = pd.read_csv('E:\Vaishnavi\gradebook.csv')
df=df.drop(' rollno',axis=1)
train_X, test_X = train_test_split(df, test_size=0.2, random_state=42)
print(len(train_X), "train +", len(test_X), "test")
df2 = train_X.copy()
le = LabelEncoder()
le.fit(df2.result)

le.classes_ 
df2.loc[:,'result'] = le.transform(df2.result)
df2.head(6)
scaler = StandardScaler()
scaler.fit(df2)
data_scaled = scaler.transform(df2)
data_scaled[0:3]
pca = PCA()
# fit PCA
pca.fit(data_scaled)
features = range(pca.n_components_) 
features 
data_pca = pca.transform(data_scaled) 
data_pca.shape 
pca.explained_variance_ratio_ 
plt.bar(features, pca.explained_variance_ratio_) 
plt.xticks(features) 
plt.ylabel('variance') 
plt.xlabel('PCA feature') 
plt.show() 
pca2 = PCA(n_components=2, svd_solver='full') 
# fit PCA 
pca2.fit(data_scaled) 
# PCA transformed data 
data_pca2 = pca2.transform(data_scaled) 
data_pca2.shape

import scipy.cluster.hierarchy as sch
dendrogram = sch.dendrogram(sch.linkage(x, method = 'ward','single','complete','average','weighted','centroid','median'))
plt.title('Dendrogam', fontsize = 20)
plt.xlabel('student')
plt.ylabel('Ecuclidean Distance')
plt.show()
from sklearn.cluster import AgglomerativeClustering

hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')
y_hc = hc.fit_predict(x)

plt.scatter(x[y_hc == 0, 0], x[y_hc == 0, 1], s = 100, c = 'pink', label = 'miser')
plt.scatter(x[y_hc == 1, 0], x[y_hc == 1, 1], s = 100, c = 'yellow', label = 'general')
plt.scatter(x[y_hc == 2, 0], x[y_hc == 2, 1], s = 100, c = 'cyan', label = 'target')
plt.scatter(x[y_hc == 3, 0], x[y_hc == 3, 1], s = 100, c = 'magenta', label = 'spendthrift')
plt.scatter(x[y_hc == 4, 0], x[y_hc == 4, 1], s = 100, c = 'orange', label = 'careful')
plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'blue' , label = 'centeroid')

plt.style.use('fivethirtyeight')
plt.title('Hierarchial Clustering', fontsize = 20)
plt.xlabel('Result')
plt.ylabel('total')
plt.legend()
plt.grid()
plt.show()

from numpy import unique
from numpy import where
data_X = data.iloc[:,[0,1]].values
# define the model
model = DBSCAN(eps=0.7, min_samples=90)
# fit model and predict clusters
yhat = model.fit_predict(data_X)
# retrieve unique clusters
clusters = unique(yhat)
# create scatter plot for samples from each cluster
for cluster in clusters:
	# get row indexes for samples with this cluster
	row_ix = where(yhat == cluster)
	# create scatter of these samples
	plt.scatter(data_X[row_ix, 0], data_X[row_ix, 1])
# show the plot
plt.show()

xs = data_pca2[:,0] 
ys = data_pca2[:,1] 
#zs = train_X.iloc[:,2] 
plt.scatter(ys, xs) 
#plt.scatter(ys, zs, c=labels) 
plt.grid(False) 
plt.title('Scatter Plot of student data') 
plt.xlabel('PCA-01') 
plt.ylabel('PCA-02') 
plt.show()


k=4 
kmeans = KMeans(n_clusters=k, init = 'k-means++',random_state = 42) 
pipeline = make_pipeline(scaler, pca2, kmeans) 
#pipeline = make_pipeline(kmeans) 
# fit the model to the scaled dataset 
model_fit = pipeline.fit(df2) 
model_fit
labels = model_fit.predict(df2) //Assigning labels 
labels
# lets add the clusters to the dataset 
train_X['Clusters'] = labels 
# Number of data points for each feature in each cluster 
train_X.groupby('Clusters').count()

xs = data_pca2[:,0] 
ys = data_pca2[:,1] 
#zs = train_X.iloc[:,2] 
plt.scatter(ys, xs,c=labels) 
#plt.scatter(ys, zs, c=labels) 
plt.grid(False) 
plt.title('Scatter Plot of student data') 
plt.xlabel('PCA-01') 
plt.ylabel('PCA-02') 
plt.show()
df_new = test_X.copy() 
# predict the labels 
le.fit(df_new.result) 
#update df2 with transformed values of gender 
df_new.loc[:,'result'] = le.transform(df_new.result) 
labels_test = model_fit.predict(df_new) 
labels_testplt.show() 
test_X['Clusters'] = labels_test 
# Number of data points for each feature in each cluster 
test_X.groupby('Clusters').count() 
query = (test_X['Clusters']==1) 
test_X[query]




